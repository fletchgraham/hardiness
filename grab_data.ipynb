{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.3 64-bit ('venv')",
   "metadata": {
    "interpreter": {
     "hash": "51fb8ebfdd4a7e9b3719894d7fc868e145a1f42a932dbcd441fffd054f8e8fb3"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Data for Creating a Plant Hardiness Map\n",
    "\n",
    "This notebook contains code for gathering and processing the data needed to create an updated map of plant hardiness zones in the US. Just a warning: this notebook takes quite a while to run.\n",
    "\n",
    "[click here to see the data plotted on an interactive map.](https://fletchgraham.github.io/hardiness/)\n",
    "\n",
    "[or here to see an Observable notebook with a voronoi zone map.](https://observablehq.com/@fletchgraham/us-plant-hardiness-zones-voronoi)\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "import tarfile\n",
    "from urllib.request import urlopen\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.ncei.noaa.gov/data/gsoy/archive/gsoy-latest.tar.gz'\n",
    "b = BytesIO(urlopen(url).read())"
   ]
  },
  {
   "source": [
    "This url gives us an archive filled to the brim with indivudual csv files. Each csv represents a weather station somewhere in the world and each record in that csv represents the summary of a particular year. Our task is to filter this data down and get it into one csv file. \n",
    "\n",
    "The code below iterates over the csv files in the archive and joins them into one pandas dataframe, ignoring them if they aren't in the US. It also drops any records that are older than 2014."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def extractor():\n",
    "    with tarfile.open(mode='r', fileobj=b) as archive:\n",
    "        for m in archive.getmembers():\n",
    "            if not m.name.startswith('US'):\n",
    "                continue\n",
    "\n",
    "            yield archive.extractfile(m)\n",
    "        \n",
    "\n",
    "df_main = pd.DataFrame(columns=['STATION','NAME', 'DATE', 'LATITUDE', 'LONGITUDE', 'EMNT'])\n",
    "\n",
    "gen = extractor()\n",
    "for g in gen:\n",
    "    df = pd.read_csv(g)\n",
    "    if not 'EMNT' in df.keys():\n",
    "        continue\n",
    "    filtered = df[['STATION','NAME', 'DATE', 'LATITUDE', 'LONGITUDE', 'EMNT']].loc[df['DATE'] >= 2014].dropna()\n",
    "    df_main = pd.concat([df_main, filtered])\n",
    "    "
   ]
  },
  {
   "source": [
    "Lets just inspect what we got out of that."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(36254, 6)"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "df_main.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "        STATION                   NAME  DATE  LATITUDE  LONGITUDE  EMNT\n",
       "41  USC00010160  ALEXANDER CITY, AL US  2014   32.9452    -85.948 -15.6\n",
       "42  USC00010160  ALEXANDER CITY, AL US  2015   32.9452    -85.948 -12.8\n",
       "43  USC00010160  ALEXANDER CITY, AL US  2016   32.9452    -85.948  -6.7\n",
       "44  USC00010160  ALEXANDER CITY, AL US  2017   32.9452    -85.948 -10.0\n",
       "45  USC00010160  ALEXANDER CITY, AL US  2018   32.9452    -85.948 -12.2"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>STATION</th>\n      <th>NAME</th>\n      <th>DATE</th>\n      <th>LATITUDE</th>\n      <th>LONGITUDE</th>\n      <th>EMNT</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>41</th>\n      <td>USC00010160</td>\n      <td>ALEXANDER CITY, AL US</td>\n      <td>2014</td>\n      <td>32.9452</td>\n      <td>-85.948</td>\n      <td>-15.6</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>USC00010160</td>\n      <td>ALEXANDER CITY, AL US</td>\n      <td>2015</td>\n      <td>32.9452</td>\n      <td>-85.948</td>\n      <td>-12.8</td>\n    </tr>\n    <tr>\n      <th>43</th>\n      <td>USC00010160</td>\n      <td>ALEXANDER CITY, AL US</td>\n      <td>2016</td>\n      <td>32.9452</td>\n      <td>-85.948</td>\n      <td>-6.7</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>USC00010160</td>\n      <td>ALEXANDER CITY, AL US</td>\n      <td>2017</td>\n      <td>32.9452</td>\n      <td>-85.948</td>\n      <td>-10.0</td>\n    </tr>\n    <tr>\n      <th>45</th>\n      <td>USC00010160</td>\n      <td>ALEXANDER CITY, AL US</td>\n      <td>2018</td>\n      <td>32.9452</td>\n      <td>-85.948</td>\n      <td>-12.2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "df_main.head()"
   ]
  },
  {
   "source": [
    "Looks good! now we write the csv file."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main.to_csv('gsoy.csv')"
   ]
  }
 ]
}